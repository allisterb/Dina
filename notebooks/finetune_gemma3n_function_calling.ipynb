{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e4d70ff4",
   "metadata": {},
   "source": [
    "# Fine-tune Gemma 3n for function calling\n",
    "This notebook is based on the [article](https://medium.com/@lucamassaron/fine-tuning-gemma-3-1b-for-function-calling-a-step-by-step-guide-66a613352f99) and [code](https://gist.github.com/lmassaron/7166f58912ff23de3fa627671fac07df) by Luca Massaron for fine-tuning the Gemma 3 1B model for function calling, together with the [notebook](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Gemma3N_(4B)-Conversational.ipynb) released by Unsloth for fine-tuning Gemma 3n models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c1bdbf01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install latest transformers for Gemma 3N\n",
    "# !pip install --no-deps transformers>=4.53.1 # Only for Gemma 3N\n",
    "# !pip install --no-deps --upgrade timm # Only for Gemma 3N\n",
    "# from huggingface_hub import login\n",
    "# login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "88815e51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      "ðŸ¦¥ Unsloth Zoo will now patch everything to make training faster!\n",
      "==((====))==  Unsloth 2025.8.1: Fast Gemma3N patching. Transformers: 4.55.0.\n",
      "   \\\\   /|    NVIDIA L4. Num GPUs = 1. Max memory: 22.034 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.7.1+cu126. CUDA: 8.9. CUDA Toolkit: 12.6. Triton: 3.3.1\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.31.post1. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n",
      "Unsloth: Gemma3N does not support SDPA - switching to eager!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "417bbf6e121c4cca9b50d0fb80c4c899",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Get the model from Unsloth\n",
    "from unsloth import FastModel\n",
    "import torch\n",
    "\n",
    "torch._dynamo.config.cache_size_limit = 64  # or higher  \n",
    "\n",
    "model, tokenizer = FastModel.from_pretrained(\n",
    "    model_name = \"unsloth/gemma-3n-E4B-it-unsloth-bnb-4bit\", # Text only but it's for Ollama so it should be fine for now\n",
    "    dtype = None, # None for auto detection\n",
    "    max_seq_length = 2048, # Used for training for function call responses\n",
    "    load_in_4bit = True,  # 4 bit quantization to reduce memory\n",
    "    full_finetuning = False, \n",
    "    attn_implementation=\"flash_attention\", \n",
    "    # token = \"hf_...\", # use one if using gated models\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "283fc968",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added 8 special tokens to the tokenizer\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Gemma3nTextScaledWordEmbedding(262408, 2048, padding_idx=0)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tokenizer doesn't have the special tokens for ChatML format, so we need to add them\n",
    "from enum import Enum\n",
    "class ChatmlSpecialTokens(str, Enum):\n",
    "    \"\"\"Enum class defining special tokens used in the ChatML format\"\"\"\n",
    "\n",
    "    tools = \"<tools>\"\n",
    "    eotools = \"</tools>\"\n",
    "    think = \"<think>\"\n",
    "    eothink = \"</think>\"\n",
    "    tool_call = \"<tool_call>\"\n",
    "    eotool_call = \"</tool_call>\"\n",
    "    tool_response = \"<tool_response>\"\n",
    "    eotool_response = \"</tool_response>\"\n",
    "    pad_token = \"<pad>\"\n",
    "    eos_token = \"<eos>\"\n",
    "\n",
    "    @classmethod\n",
    "    def list(cls):\n",
    "        return [c.value for c in cls]\n",
    "\n",
    "    @classmethod\n",
    "    def dict(cls):\n",
    "        d = dict()\n",
    "        for c in cls:\n",
    "            d[c.name] = c.value\n",
    "        return d\n",
    "    \n",
    "\n",
    "special_tokens_dict = {'additional_special_tokens': ChatmlSpecialTokens.list()}\n",
    "\n",
    "# This currently crashes with Unsloth\n",
    "\n",
    "# Add the special tokens to the tokenizer\n",
    "# a = tokenizer.tokenizer.add_special_tokens(special_tokens_dict)\n",
    "# print(f'Added {a} special tokens to the tokenizer')\n",
    "\n",
    "\n",
    "# Set the pad token\n",
    "# tokenizer.pad_token = ChatmlSpecialTokens.pad_token.value\n",
    "# Resize the model's token embeddings to accommodate the new tokens\n",
    "# model.resize_token_embeddings(len(tokenizer.tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d7380e05",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth: Dropout = 0 is supported for fast patching. You are using dropout = 0.05.\n",
      "Unsloth will patch all other layers, except LoRA matrices, causing a performance hit.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Making `model.base_model.model.model.language_model` require gradients\n"
     ]
    }
   ],
   "source": [
    "# Get the finetuning model\n",
    "from peft import TaskType\n",
    "lora_arguments = {\n",
    "    \"r\": 16,\n",
    "    \"lora_alpha\": 64,\n",
    "    \"lora_dropout\": 0.05,\n",
    "}\n",
    "model = FastModel.get_peft_model(\n",
    "    model,\n",
    "    finetune_vision_layers     = False, # Turn off for just text!\n",
    "    finetune_language_layers   = True,  # Should leave on!\n",
    "    finetune_attention_modules = True,  # Attention good for GRPO\n",
    "    finetune_mlp_modules       = True,  # Should leave on always!\n",
    "    **lora_arguments,\n",
    "    task_type = TaskType.CAUSAL_LM, \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0a059254",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use this chat template for training tool calls\n",
    "tokenizer.chat_template = (\n",
    "    \"{{ bos_token }}{% for message in messages %}{% if message['role'] != 'system' %}{{ '<start_of_turn>' + message['role'] + '\\n' + message['content'] | trim + '<end_of_turn><eos>\\n' }}{% endif %}{% endfor %}{% if add_generation_prompt %}{{'<start_of_turn>model\\n'}}{% endif %}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "33d5b0c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'conversations': [{'content': \"You are a function calling AI model. You are provided with function signatures within <tools></tools> XML tags.You may call one or more functions to assist with the user query. Don't make assumptions about what values to plug into functions.Here are the available tools:<tools> [{'type': 'function', 'function': {'name': 'search_book', 'description': 'Search for a book based on title and/or author', 'parameters': {'type': 'object', 'properties': {'title': {'type': 'string', 'description': 'The title of the book'}, 'author': {'type': 'string', 'description': 'The author of the book'}}, 'required': []}}}, {'type': 'function', 'function': {'name': 'get_definition', 'description': 'Get the definition of a word', 'parameters': {'type': 'object', 'properties': {'word': {'type': 'string', 'description': 'The word to get the definition for'}}, 'required': ['word']}}}] </tools>Use the following pydantic model json schema for each tool call you will make: {'title': 'FunctionCall', 'type': 'object', 'properties': {'arguments': {'title': 'Arguments', 'type': 'object'}, 'name': {'title': 'Name', 'type': 'string'}}, 'required': ['arguments', 'name']}For each function call return a json object with function name and arguments within <tool_call></tool_call> XML tags as follows:\\n<tool_call>\\n{tool_call}\\n</tool_call>Also, before making a call to a function take the time to plan the function to take. Make that thinking process between <think>{your thoughts}</think>\\n\\nCan you help me find a book?\",\n",
       "   'role': 'human'},\n",
       "  {'content': 'Of course! Could you please provide me with the title or the author of the book?',\n",
       "   'role': 'model'},\n",
       "  {'content': 'The book is called \"1984\" and it\\'s written by George Orwell.',\n",
       "   'role': 'human'},\n",
       "  {'content': \"<tool_call>\\n{'name': 'search_book', 'arguments': {'title': '1984', 'author': 'George Orwell'}}\\n</tool_call>\",\n",
       "   'role': 'model'},\n",
       "  {'content': \"<tool_response>\\n{'status': 'success', 'data': {'title': '1984', 'author': 'George Orwell', 'publisher': 'Secker & Warburg', 'publication_year': '1949', 'genre': 'Dystopian, political fiction, social science fiction'}}\\n</tool_response>\",\n",
       "   'role': 'tool'},\n",
       "  {'content': 'I found the book you were looking for. \"1984\" by George Orwell was published by Secker & Warburg in 1949. It falls under the genres of Dystopian, political fiction, and social science fiction.',\n",
       "   'role': 'model'}]}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the datasets\n",
    "from datasets import load_dataset\n",
    "dataset = load_dataset(\"lmassaron/hermes-function-calling-v1\", split=\"train\")\n",
    "eval_dataset = load_dataset(\"lmassaron/hermes-function-calling-v1\", split=\"test[:100]\")\n",
    "dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d6251aea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "38f6166f428f465287900ae23bc73c44",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/100 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'conversations': [{'content': 'You are a helpful assistant, with no access to external functions.Also, before making a call to a function take the time to plan the function to take. Make that thinking process between <think>{your thoughts}</think>\\n\\nTransform these sentences into a passive voice\\nThey are singing a song.',\n",
       "   'role': 'human'},\n",
       "  {'content': 'A song is being sung by them.', 'role': 'model'},\n",
       "  {'content': 'What is passive voice and why is it used?', 'role': 'human'},\n",
       "  {'content': 'In passive voice, the subject of a sentence receives the action instead of performing it. Passive voice is used to shift focus from the doer (subject) of an action to the receiver (object). It can also be used to make writing more formal or objective.',\n",
       "   'role': 'model'},\n",
       "  {'content': 'How can I identify passive voice in a sentence?',\n",
       "   'role': 'human'},\n",
       "  {'content': 'Passive voice can be identified by looking for a form of \"to be\" (is, was, are, were, be, being, been) followed by a past participle verb (usually ending in -ed or -en) and the absence of an explicit doer (or when the doer is introduced with \"by\"). For example, \"The cake was baked by John\" is in passive voice because \"was\" is a form of \"to be,\" \"baked\" is a past participle verb, and \"John\" is introduced with \"by.\"',\n",
       "   'role': 'model'},\n",
       "  {'content': 'When should I avoid using passive voice in my writing?',\n",
       "   'role': 'human'},\n",
       "  {'content': 'Passive voice can be overused and make writing sound unnatural or unclear. Active voice is generally preferred for clear, direct communication. However, there are some cases where passive voice may be appropriate or necessary, such as when the doer is unknown, unimportant, or purposely left out. Additionally, passive voice can be used to vary sentence structure and create emphasis on the object or receiver of an action.',\n",
       "   'role': 'model'}],\n",
       " 'text': '<start_of_turn>human\\nYou are a helpful assistant, with no access to external functions.Also, before making a call to a function take the time to plan the function to take. Make that thinking process between <think>{your thoughts}</think>\\n\\nTransform these sentences into a passive voice\\nThey are singing a song.<end_of_turn><eos>\\n<start_of_turn>model\\nA song is being sung by them.<end_of_turn><eos>\\n<start_of_turn>human\\nWhat is passive voice and why is it used?<end_of_turn><eos>\\n<start_of_turn>model\\nIn passive voice, the subject of a sentence receives the action instead of performing it. Passive voice is used to shift focus from the doer (subject) of an action to the receiver (object). It can also be used to make writing more formal or objective.<end_of_turn><eos>\\n<start_of_turn>human\\nHow can I identify passive voice in a sentence?<end_of_turn><eos>\\n<start_of_turn>model\\nPassive voice can be identified by looking for a form of \"to be\" (is, was, are, were, be, being, been) followed by a past participle verb (usually ending in -ed or -en) and the absence of an explicit doer (or when the doer is introduced with \"by\"). For example, \"The cake was baked by John\" is in passive voice because \"was\" is a form of \"to be,\" \"baked\" is a past participle verb, and \"John\" is introduced with \"by.\"<end_of_turn><eos>\\n<start_of_turn>human\\nWhen should I avoid using passive voice in my writing?<end_of_turn><eos>\\n<start_of_turn>model\\nPassive voice can be overused and make writing sound unnatural or unclear. Active voice is generally preferred for clear, direct communication. However, there are some cases where passive voice may be appropriate or necessary, such as when the doer is unknown, unimportant, or purposely left out. Additionally, passive voice can be used to vary sentence structure and create emphasis on the object or receiver of an action.<end_of_turn><eos>\\n'}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert the dataset to the correct format for finetuning\n",
    "def formatting_prompts_func(examples):\n",
    "   convos = examples[\"conversations\"]\n",
    "   texts = [tokenizer.apply_chat_template(convo, tokenize = False, add_generation_prompt = False).removeprefix('<bos>') for convo in convos]\n",
    "   return { \"text\" : texts, }\n",
    "\n",
    "# Convert the dataset to the correct format for finetuning\n",
    "def formatting_eval_prompts_func(examples):\n",
    "   convos = examples[\"conversations\"]\n",
    "   texts = [tokenizer.apply_chat_template(convo, tokenize = False, add_generation_prompt = False).removeprefix('<bos>') for convo in convos]\n",
    "   return { \"text\" : texts, }\n",
    "dataset =  dataset.map(formatting_prompts_func, batched = True)\n",
    "eval_dataset =  eval_dataset.map(formatting_prompts_func, batched = True)\n",
    "eval_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ee460c5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup the fine-tuning trainer\n",
    "from unsloth import is_bfloat16_supported\n",
    "training_arguments = {\n",
    "    # Basic training configuration\n",
    "    #\"num_train_epochs\": 1,\n",
    "    \"bf16\": is_bfloat16_supported(),  # Use bfloat16 if supported\n",
    "    \"fp16\": not is_bfloat16_supported(),  # Use if bfloat16 not supported\n",
    "    \"max_steps\": 240,\n",
    "    # Evaluation and saving\n",
    "    \"eval_strategy\": \"epoch\",\n",
    "    \"save_strategy\": \"epoch\",\n",
    "    \"per_device_train_batch_size\": 1,\n",
    "    \"per_device_eval_batch_size\": 1,\n",
    "    \"gradient_accumulation_steps\": 4,\n",
    "    # Optimization settings\n",
    "    \"optim\": \"adamw_torch_fused\",\n",
    "    \"learning_rate\": 1e-4,\n",
    "    \"weight_decay\": 0.1,\n",
    "    \"max_grad_norm\": 1.0,\n",
    "    \"lr_scheduler_type\": \"linear\",\n",
    "    \"warmup_ratio\": 0.1,\n",
    "    # Memory optimization\n",
    "    \"gradient_checkpointing\": True,\n",
    "    \"gradient_checkpointing_kwargs\": {\"use_reentrant\": False},\n",
    "    \"save_total_limit\": 2,\n",
    "    \"greater_is_better\": False,\n",
    "    # Logging and output\n",
    "    \"logging_steps\": 1,\n",
    "    #\"report_to\": None,\n",
    "    \"logging_dir\": \"logs/runs\",\n",
    "    \"overwrite_output_dir\": True,\n",
    "    # Model sharing\n",
    "    \"push_to_hub\": False,\n",
    "    \"hub_private_repo\": False,\n",
    "}\n",
    "\n",
    "from trl import SFTTrainer, SFTConfig\n",
    "trainer = SFTTrainer(\n",
    "    model = model,\n",
    "    tokenizer = tokenizer,\n",
    "    train_dataset = dataset,\n",
    "    eval_dataset= eval_dataset,\n",
    "    dataset_text_field = \"text\",\n",
    "    max_seq_length = 2048,\n",
    "    dataset_num_proc = 2,\n",
    "    packing = True,\n",
    "    bias = \"none\",\n",
    "    batch_size=24,\n",
    "    args = SFTConfig(**training_arguments),  \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "75937357",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f985bddf66d94c4bac3f9ef742439688",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=8):   0%|          | 0/100 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Train the model on the responses only, ignore user instructions\n",
    "from unsloth.chat_templates import train_on_responses_only\n",
    "trainer = train_on_responses_only(\n",
    "    trainer,\n",
    "    instruction_part = \"<start_of_turn>user\\n\",\n",
    "    response_part = \"<start_of_turn>model\\n\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d9c2737e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"<bos><start_of_turn>human\\nYou are a function calling AI model. You are provided with function signatures within <tools></tools> XML tags.You may call one or more functions to assist with the user query. Don't make assumptions about what values to plug into functions.Here are the available tools:<tools> [{'type': 'function', 'function': {'name': 'analyze_website', 'description': 'Analyze the content and structure of a website', 'parameters': {'type': 'object', 'properties': {'url': {'type': 'string', 'description': 'The URL of the website to analyze'}}, 'required': ['url']}}}, {'type': 'function', 'function': {'name': 'calculate_bmi', 'description': 'Calculate the Body Mass Index (BMI)', 'parameters': {'type': 'object', 'properties': {'weight': {'type': 'number', 'description': 'The weight in kilograms'}, 'height': {'type': 'number', 'description': 'The height in meters'}}, 'required': ['weight', 'height']}}}] </tools>Use the following pydantic model json schema for each tool call you will make: {'title': 'FunctionCall', 'type': 'object', 'properties': {'arguments': {'title': 'Arguments', 'type': 'object'}, 'name': {'title': 'Name', 'type': 'string'}}, 'required': ['arguments', 'name']}For each function call return a json object with function name and arguments within <tool_call></tool_call> XML tags as follows:\\n<tool_call>\\n{tool_call}\\n</tool_call>Also, before making a call to a function take the time to plan the function to take. Make that thinking process between <think>{your thoughts}</think>\\n\\nHi, I need to analyze the structure and content of a website. Can you help me with that?<end_of_turn><eos>\\n<start_of_turn>model\\nOf course, I can help you with that. Could you please provide me with the URL of the website you want to analyze?<end_of_turn><eos>\\n<start_of_turn>human\\nSure, the website URL is www.example.com.<end_of_turn><eos>\\n<start_of_turn>model\\n<tool_call>\\n{'name': 'analyze_website', 'arguments': {'url': 'www.example.com'}}\\n</tool_call><end_of_turn><eos>\\n<start_of_turn>tool\\n<tool_response>\\n{'status': 'success', 'message': 'Website analysis completed', 'data': {'structure': 'The website has a clear and intuitive structure with a navigation menu at the top. The homepage, about us, services, and contact us pages are easily accessible.', 'content': 'The content is well-written and informative. It is updated regularly and is relevant to the target audience.'}}\\n</tool_response><end_of_turn><eos>\\n<start_of_turn>model\\nThe analysis of the website www.example.com is completed. The website has a clear and intuitive structure with a navigation menu at the top. The homepage, about us, services, and contact us pages are easily accessible. The content is well-written and informative. It is updated regularly and is relevant to the target audience.<end_of_turn><eos>\\n\""
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Verify the chat template was applied correctly. Only 1 <bos> token should be present.\n",
    "tokenizer.decode(trainer.train_dataset[100][\"input_ids\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d0a4bf65",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"                                                                                                                                                                                                                                                                                                                                                                                                                    Of course, I can help you with that. Could you please provide me with the URL of the website you want to analyze?<end_of_turn><eos>\\n<start_of_turn>human\\nSure, the website URL is www.example.com.<end_of_turn><eos>\\n<start_of_turn>model\\n<tool_call>\\n{'name': 'analyze_website', 'arguments': {'url': 'www.example.com'}}\\n</tool_call><end_of_turn><eos>\\n<start_of_turn>tool\\n<tool_response>\\n{'status': 'success', 'message': 'Website analysis completed', 'data': {'structure': 'The website has a clear and intuitive structure with a navigation menu at the top. The homepage, about us, services, and contact us pages are easily accessible.', 'content': 'The content is well-written and informative. It is updated regularly and is relevant to the target audience.'}}\\n</tool_response><end_of_turn><eos>\\n<start_of_turn>model\\nThe analysis of the website www.example.com is completed. The website has a clear and intuitive structure with a navigation menu at the top. The homepage, about us, services, and contact us pages are easily accessible. The content is well-written and informative. It is updated regularly and is relevant to the target audience.<end_of_turn><eos>\\n\""
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Verify user instruction masked\n",
    "tokenizer.decode([tokenizer.pad_token_id if x == -100 else x for x in trainer.train_dataset[100][\"labels\"]]).replace(tokenizer.pad_token, \" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "88247f1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n",
      "   \\\\   /|    Num examples = 4,167 | Num Epochs = 1 | Total steps = 240\n",
      "O^O/ \\_/ \\    Batch size per device = 1 | Gradient accumulation steps = 4\n",
      "\\        /    Data Parallel GPUs = 1 | Total batch size (1 x 4 x 1) = 4\n",
      " \"-____-\"     Trainable parameters = 38,420,480 of 7,888,398,672 (0.49% trained)\n",
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='240' max='240' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [240/240 28:08, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.691600</td>\n",
       "      <td>0.393198</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth: Not an error, but Gemma3nForConditionalGeneration does not accept `num_items_in_batch`.\n",
      "Using gradient accumulation will be very slightly less accurate.\n",
      "Read more on gradient accumulation issues here: https://unsloth.ai/blog/gradient\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "trainer_stats = trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0a36c001",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'conversations': [{'content': \"You are a function calling AI model. You are provided with function signatures within <tools></tools> XML tags.You may call one or more functions to assist with the user query. Don't make assumptions about what values to plug into functions.Here are the available tools:<tools> [{'type': 'function', 'function': {'name': 'create_todo', 'description': 'Create a new todo item', 'parameters': {'type': 'object', 'properties': {'title': {'type': 'string', 'description': 'The title of the todo item'}, 'description': {'type': 'string', 'description': 'The description of the todo item'}}, 'required': ['title']}}}, {'type': 'function', 'function': {'name': 'create_invoice', 'description': 'Create a new invoice', 'parameters': {'type': 'object', 'properties': {'client': {'type': 'string', 'description': 'The name of the client'}, 'items': {'type': 'array', 'items': {'type': 'object', 'properties': {'description': {'type': 'string', 'description': 'The description of the item'}, 'quantity': {'type': 'integer', 'description': 'The quantity of the item'}, 'price': {'type': 'number', 'description': 'The price of the item'}}, 'required': ['description', 'quantity', 'price']}, 'description': 'The list of items in the invoice'}}, 'required': ['client', 'items']}}}] </tools>Use the following pydantic model json schema for each tool call you will make: {'title': 'FunctionCall', 'type': 'object', 'properties': {'arguments': {'title': 'Arguments', 'type': 'object'}, 'name': {'title': 'Name', 'type': 'string'}}, 'required': ['arguments', 'name']}For each function call return a json object with function name and arguments within <tool_call></tool_call> XML tags as follows:\\n<tool_call>\\n{tool_call}\\n</tool_call>Also, before making a call to a function take the time to plan the function to take. Make that thinking process between <think>{your thoughts}</think>\\n\\nI need to create a new todo item.\",\n",
       "   'role': 'human'},\n",
       "  {'content': 'Sure, I can help with that. Could you please provide me with the title and description of the todo item?',\n",
       "   'role': 'model'},\n",
       "  {'content': 'The title is \"Grocery Shopping\" and the description is \"Buy fruits, vegetables, and bread\".',\n",
       "   'role': 'human'},\n",
       "  {'content': \"<tool_call>\\n{'name': 'create_todo', 'arguments': {'title': 'Grocery Shopping', 'description': 'Buy fruits, vegetables, and bread'}}\\n</tool_call>\",\n",
       "   'role': 'model'},\n",
       "  {'content': '<tool_response>\\n{\\'status\\': \\'success\\', \\'message\\': \"Todo item \\'Grocery Shopping\\' has been created successfully.\"}\\n</tool_response>',\n",
       "   'role': 'tool'},\n",
       "  {'content': 'Your todo item \"Grocery Shopping\" has been created successfully. It includes \"Buy fruits, vegetables, and bread\".',\n",
       "   'role': 'model'}],\n",
       " 'text': '<start_of_turn>human\\nYou are a function calling AI model. You are provided with function signatures within <tools></tools> XML tags.You may call one or more functions to assist with the user query. Don\\'t make assumptions about what values to plug into functions.Here are the available tools:<tools> [{\\'type\\': \\'function\\', \\'function\\': {\\'name\\': \\'create_todo\\', \\'description\\': \\'Create a new todo item\\', \\'parameters\\': {\\'type\\': \\'object\\', \\'properties\\': {\\'title\\': {\\'type\\': \\'string\\', \\'description\\': \\'The title of the todo item\\'}, \\'description\\': {\\'type\\': \\'string\\', \\'description\\': \\'The description of the todo item\\'}}, \\'required\\': [\\'title\\']}}}, {\\'type\\': \\'function\\', \\'function\\': {\\'name\\': \\'create_invoice\\', \\'description\\': \\'Create a new invoice\\', \\'parameters\\': {\\'type\\': \\'object\\', \\'properties\\': {\\'client\\': {\\'type\\': \\'string\\', \\'description\\': \\'The name of the client\\'}, \\'items\\': {\\'type\\': \\'array\\', \\'items\\': {\\'type\\': \\'object\\', \\'properties\\': {\\'description\\': {\\'type\\': \\'string\\', \\'description\\': \\'The description of the item\\'}, \\'quantity\\': {\\'type\\': \\'integer\\', \\'description\\': \\'The quantity of the item\\'}, \\'price\\': {\\'type\\': \\'number\\', \\'description\\': \\'The price of the item\\'}}, \\'required\\': [\\'description\\', \\'quantity\\', \\'price\\']}, \\'description\\': \\'The list of items in the invoice\\'}}, \\'required\\': [\\'client\\', \\'items\\']}}}] </tools>Use the following pydantic model json schema for each tool call you will make: {\\'title\\': \\'FunctionCall\\', \\'type\\': \\'object\\', \\'properties\\': {\\'arguments\\': {\\'title\\': \\'Arguments\\', \\'type\\': \\'object\\'}, \\'name\\': {\\'title\\': \\'Name\\', \\'type\\': \\'string\\'}}, \\'required\\': [\\'arguments\\', \\'name\\']}For each function call return a json object with function name and arguments within <tool_call></tool_call> XML tags as follows:\\n<tool_call>\\n{tool_call}\\n</tool_call>Also, before making a call to a function take the time to plan the function to take. Make that thinking process between <think>{your thoughts}</think>\\n\\nI need to create a new todo item.<end_of_turn><eos>\\n<start_of_turn>model\\nSure, I can help with that. Could you please provide me with the title and description of the todo item?<end_of_turn><eos>\\n<start_of_turn>human\\nThe title is \"Grocery Shopping\" and the description is \"Buy fruits, vegetables, and bread\".<end_of_turn><eos>\\n<start_of_turn>model\\n<tool_call>\\n{\\'name\\': \\'create_todo\\', \\'arguments\\': {\\'title\\': \\'Grocery Shopping\\', \\'description\\': \\'Buy fruits, vegetables, and bread\\'}}\\n</tool_call><end_of_turn><eos>\\n<start_of_turn>tool\\n<tool_response>\\n{\\'status\\': \\'success\\', \\'message\\': \"Todo item \\'Grocery Shopping\\' has been created successfully.\"}\\n</tool_response><end_of_turn><eos>\\n<start_of_turn>model\\nYour todo item \"Grocery Shopping\" has been created successfully. It includes \"Buy fruits, vegetables, and bread\".<end_of_turn><eos>\\n'}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_dataset[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6a6b7d3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tool_call>\n",
      "{'name': 'create_todo', 'arguments': {'title': 'Grocery Shopping', 'description': 'Buy milk, bread, and eggs'}}\n",
      "</tool_call><end_of_turn>\n"
     ]
    }
   ],
   "source": [
    "from transformers import TextStreamer\n",
    "\n",
    "messages = [{\n",
    "    \"role\": \"user\",\n",
    "    \"content\": [{\n",
    "        \"type\" : \"text\",\n",
    "        \"text\" : eval_dataset[10][\"conversations\"][0][\"content\"],\n",
    "    }]\n",
    "}]\n",
    "# Convert the messages to the correct format for generation\n",
    "inputs = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    add_generation_prompt = True, # Must add for generation\n",
    "    return_tensors = \"pt\",\n",
    "    tokenize = True,\n",
    "    return_dict = True,\n",
    ").to(\"cuda\")\n",
    "\n",
    "_ = model.generate(\n",
    "    **inputs,\n",
    "    max_new_tokens = 64, # Increase for longer outputs!\n",
    "    # Recommended Gemma-3 settings!\n",
    "    temperature = 1.0, top_p = 0.95, top_k = 64,\n",
    "    streamer = TextStreamer(tokenizer, skip_prompt = True),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2e960965",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "model_name = \"gemma3n_e4b_tools_test\"\n",
    "model_name_gguf = model_name + \"-GGUF\"\n",
    "model_merged_hf_repo = \"allisterb/gemma3n_e4b_tools_test\"\n",
    "model_gguf_hf_repo = model_merged_hf_repo + \"-GGUF\"\n",
    "hf_token = os.environ[\"HF_TOKEN\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b44a5ba4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found HuggingFace hub cache directory: /home/eddsa-key-20250707/.cache/huggingface/hub\n",
      "Checking cache directory for required files...\n",
      "Cache check failed: model-00001-of-00004.safetensors not found in local cache.\n",
      "Not all required files found in cache. Will proceed with downloading.\n",
      "Downloading safetensors index for unsloth/gemma-3n-e4b-it...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth: Merging weights into 16bit:   0%|          | 0/4 [00:00<?, ?it/s]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b6f1d2ee72a430a94c27094532ee493",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00004.safetensors:   0%|          | 0.00/3.08G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth: Merging weights into 16bit:  25%|â–ˆâ–ˆâ–Œ       | 1/4 [00:38<01:55, 38.40s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af856b2b6ce34a519b1db53557ccd231",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00004.safetensors:   0%|          | 0.00/4.97G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth: Merging weights into 16bit:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 2/4 [01:51<01:57, 58.91s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "17033d20de724595ad514512f5992841",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00003-of-00004.safetensors:   0%|          | 0.00/4.99G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth: Merging weights into 16bit:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 3/4 [03:10<01:08, 68.16s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c5a44811bae45038df2ebed591fac32",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00004-of-00004.safetensors:   0%|          | 0.00/2.66G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth: Merging weights into 16bit: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [03:45<00:00, 56.37s/it]\n"
     ]
    }
   ],
   "source": [
    "# Save merged model\n",
    "model.save_pretrained_merged(model_name, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "26407b30",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No files have been modified since last commit. Skipping to prevent empty commit.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b10d1f91383a410a841f56e6c2a3ec99",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing Files (0 / 0)                : |          |  0.00B /  0.00B            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d18d5b865dfa45cc877e4f0587e50996",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "New Data Upload                         : |          |  0.00B /  0.00B            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0216ed7a3db6483a84bb51cb44efe079",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  /tmp/tmptcrx_s_7/tokenizer.model      : 100%|##########| 4.70MB / 4.70MB            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9aad815dfaaf44ae9b867abc0f115d40",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  /tmp/tmptcrx_s_7/tokenizer.json       : 100%|##########| 33.4MB / 33.4MB            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No files have been modified since last commit. Skipping to prevent empty commit.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found HuggingFace hub cache directory: /home/eddsa-key-20250707/.cache/huggingface/hub\n",
      "Checking cache directory for required files...\n",
      "Cache check failed: model-00001-of-00004.safetensors not found in local cache.\n",
      "Not all required files found in cache. Will proceed with downloading.\n",
      "Downloading safetensors index for unsloth/gemma-3n-e4b-it...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "28d303906747474897b00d280cc07364",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No files have been modified since last commit. Skipping to prevent empty commit.\n",
      "Unsloth: Merging weights into 16bit:   0%|          | 0/4 [00:00<?, ?it/s]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "18e9e08d188246c59148cafa5c8ed5f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00004.safetensors:   0%|          | 0.00/3.08G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2776f53707014791bea537ec9cda9607",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing Files (0 / 0)                : |          |  0.00B /  0.00B            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "25075df124e04250a951e49dac694a63",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "New Data Upload                         : |          |  0.00B /  0.00B            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b1c110a3795a4c66a113df692e16b5e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  ..._7/model-00001-of-00004.safetensors:   1%|1         | 41.8MB / 3.08GB            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No files have been modified since last commit. Skipping to prevent empty commit.\n",
      "Unsloth: Merging weights into 16bit:  25%|â–ˆâ–ˆâ–Œ       | 1/4 [01:11<03:33, 71.28s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c5f2bf6cc1ba499aacf34e85f66b58e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00004.safetensors:   0%|          | 0.00/4.97G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d68a7f12da924fccb394b12f9ecad6ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing Files (0 / 0)                : |          |  0.00B /  0.00B            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8bb6693c7a8b47c1aa0b7863cced826a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "New Data Upload                         : |          |  0.00B /  0.00B            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "970f15243cea485da65559c773c63112",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  ..._7/model-00002-of-00004.safetensors:   1%|          | 25.1MB / 4.97GB            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth: Merging weights into 16bit:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 2/4 [03:14<03:24, 102.12s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d7f76b37ea734b2184d8cbbca4e2f39b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00003-of-00004.safetensors:   0%|          | 0.00/4.99G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad29bf187b70482893a3ee9d7b5cb7a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing Files (0 / 0)                : |          |  0.00B /  0.00B            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f730ceff665e4ca4af37e2dbe060b585",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "New Data Upload                         : |          |  0.00B /  0.00B            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8a832edb90704100a86f44c38999ff00",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  ..._7/model-00003-of-00004.safetensors:   0%|          | 1.20MB / 4.99GB            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth: Merging weights into 16bit:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 3/4 [05:28<01:56, 116.48s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "918864586d0747e686f6c4408aec7221",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00004-of-00004.safetensors:   0%|          | 0.00/2.66G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "843a7bf7b99f4721ac4ad88d15951089",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing Files (0 / 0)                : |          |  0.00B /  0.00B            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca5858b35d3548f88ff9efdf1fd558c1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "New Data Upload                         : |          |  0.00B /  0.00B            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "20eed7547dab4833a2e1348e3c41963b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  ..._7/model-00004-of-00004.safetensors:   0%|          | 36.1kB / 2.66GB            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth: Merging weights into 16bit: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [06:34<00:00, 98.52s/it] \n"
     ]
    }
   ],
   "source": [
    "model.push_to_hub_merged(model_merged_hf_repo, tokenizer, token = hf_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08da9351",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth GGUF:hf-to-gguf:Loading model: gemma3n_e4b_tools_test-GGUF\n",
      "Unsloth GGUF:hf-to-gguf:Model architecture: Gemma3nForConditionalGeneration\n",
      "Unsloth GGUF:gguf.gguf_writer:gguf: This GGUF file is for Little Endian only\n",
      "Unsloth GGUF:hf-to-gguf:Exporting model...\n",
      "Unsloth GGUF:hf-to-gguf:gguf: loading model weight map from 'model.safetensors.index.json'\n",
      "Unsloth GGUF:hf-to-gguf:gguf: loading model part 'model-00001-of-00004.safetensors'\n",
      "Unsloth GGUF:hf-to-gguf:altup_proj.weight,                 torch.bfloat16 --> Q8_0, shape = {2048, 2048, 3}\n",
      "Unsloth GGUF:hf-to-gguf:altup_unembd_proj.weight,          torch.bfloat16 --> Q8_0, shape = {2048, 2048, 3}\n",
      "Unsloth GGUF:hf-to-gguf:token_embd.weight,                 torch.bfloat16 --> Q8_0, shape = {2048, 262144}\n",
      "Unsloth GGUF:hf-to-gguf:gguf: loading model part 'model-00002-of-00004.safetensors'\n",
      "Unsloth GGUF:hf-to-gguf:per_layer_token_embd.weight,       torch.bfloat16 --> Q8_0, shape = {8960, 262144}\n",
      "Unsloth GGUF:hf-to-gguf:output_norm.weight,                torch.bfloat16 --> F32, shape = {2048}\n",
      "Unsloth GGUF:hf-to-gguf:per_layer_model_proj.weight,       torch.bfloat16 --> Q8_0, shape = {2048, 8960}\n",
      "Unsloth GGUF:hf-to-gguf:per_layer_proj_norm.weight,        torch.bfloat16 --> F32, shape = {256}\n",
      "Unsloth GGUF:hf-to-gguf:gguf: loading model part 'model-00003-of-00004.safetensors'\n",
      "Unsloth GGUF:hf-to-gguf:gguf: loading model part 'model-00004-of-00004.safetensors'\n",
      "Unsloth GGUF:hf-to-gguf:Set meta model\n",
      "Unsloth GGUF:hf-to-gguf:Set model parameters\n",
      "Unsloth GGUF:hf-to-gguf:Set model quantization version\n",
      "Unsloth GGUF:hf-to-gguf:Set model tokenizer\n",
      "Unsloth GGUF:gguf.vocab:Setting special token type bos to 2\n",
      "Unsloth GGUF:gguf.vocab:Setting special token type eos to 106\n",
      "Unsloth GGUF:gguf.vocab:Setting special token type unk to 3\n",
      "Unsloth GGUF:gguf.vocab:Setting special token type pad to 0\n",
      "Unsloth GGUF:gguf.vocab:Setting add_bos_token to True\n",
      "Unsloth GGUF:gguf.vocab:Setting add_sep_token to False\n",
      "Unsloth GGUF:gguf.vocab:Setting add_eos_token to False\n",
      "Unsloth GGUF:gguf.vocab:Setting chat_template to {{ bos_token }}{% for message in messages %}{% if message['role'] != 'system' %}{{ '<start_of_turn>' + message['role'] + '\n",
      "' + message['content'] | trim + '<end_of_turn><eos>\n",
      "' }}{% endif %}{% endfor %}{% if add_generation_prompt %}{{'<start_of_turn>model\n",
      "'}}{% endif %}\n",
      "Unsloth GGUF:gguf.gguf_writer:Writing the following files:\n",
      "Unsloth GGUF:gguf.gguf_writer:gemma3n_e4b_tools_test-GGUF.Q8_0.gguf: n_tensors = 847, total_size = 7.3G\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "66fb19a6613443878994d53b111716a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Unsloth: GGUF conversion:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth GGUF:hf-to-gguf:Model successfully exported to ./\n",
      "Unsloth: Converted to gemma3n_e4b_tools_test-GGUF.Q8_0.gguf with size = 7.3G\n",
      "Unsloth: Successfully saved GGUF to:\n",
      "gemma3n_e4b_tools_test-GGUF.Q8_0.gguf\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['gemma3n_e4b_tools_test-GGUF.Q8_0.gguf']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save to GGUF format\n",
    "model.save_pretrained_gguf(\n",
    "    model_name_gguf,\n",
    "    \"Q8_0\", # For now only Q8_0, BF16, F16 supported\n",
    "    tokenizer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edc97a7d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c666a32b43cd4c02bd2244afca858be6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing Files (0 / 0)                : |          |  0.00B /  0.00B            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d9aaba86d29410196daf7dbb3ef3cd0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "New Data Upload                         : |          |  0.00B /  0.00B            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c158db107044b5b84d99d1f3f69370e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  gemma3n_e4b_tools_test-GGUF.Q8_0.gguf :   1%|          | 41.5MB / 7.30GB            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/allisterb/gemma3n_e4b_tools_test-GGUF/commit/22454a03fdd51236b7f2f1bae97f40908de630d3', commit_message='(Trained with Unsloth)', commit_description='', oid='22454a03fdd51236b7f2f1bae97f40908de630d3', pr_url=None, repo_url=RepoUrl('https://huggingface.co/allisterb/gemma3n_e4b_tools_test-GGUF', endpoint='https://huggingface.co', repo_type='model', repo_id='allisterb/gemma3n_e4b_tools_test-GGUF'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Upload GGUF model to Hugging Face\n",
    "from huggingface_hub import HfApi\n",
    "hf_api = HfApi()\n",
    "hf_api.upload_file(\n",
    "                path_or_fileobj = model_name_gguf +\".Q8_0.gguf\",\n",
    "                path_in_repo    = model_name_gguf +\".Q8_0.gguf\",\n",
    "                repo_id         = model_gguf_hf_repo,\n",
    "                repo_type       = \"model\",\n",
    "                commit_message  = \"(Trained with Unsloth)\",\n",
    "                token = hf_token,\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15859de2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No files have been modified since last commit. Skipping to prevent empty commit.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/allisterb/gemma3n_e4b_tools_test-GGUF/commit/03dd2fddd3d881870ac6352d7c46ae5cc2fc0522', commit_message='Upload template with huggingface_hub', commit_description='', oid='03dd2fddd3d881870ac6352d7c46ae5cc2fc0522', pr_url=None, repo_url=RepoUrl('https://huggingface.co/allisterb/gemma3n_e4b_tools_test-GGUF', endpoint='https://huggingface.co', repo_type='model', repo_id='allisterb/gemma3n_e4b_tools_test-GGUF'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Upload Ollama template to Hugging Face Hub\n",
    "hf_api.upload_file(\n",
    "    path_or_fileobj=\"template\",\n",
    "    path_in_repo=\"template\",\n",
    "    repo_id=model_gguf_hf_repo,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dina",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
