{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e4d70ff4",
   "metadata": {},
   "source": [
    "# Fine-tune Gemma 3n for function calling\n",
    "This notebook is based on the [article](https://medium.com/@lucamassaron/fine-tuning-gemma-3-1b-for-function-calling-a-step-by-step-guide-66a613352f99) and [code](https://gist.github.com/lmassaron/7166f58912ff23de3fa627671fac07df) by Luca Massaron for fine-tuning the Gemma 3 1B model for function calling, together with the [notebook](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Gemma3N_(4B)-Conversational.ipynb) released by Unsloth for fine-tuning Gemma 3n models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c1bdbf01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install latest transformers for Gemma 3N\n",
    "# !pip install --no-deps transformers>=4.53.1 # Only for Gemma 3N\n",
    "# !pip install --no-deps --upgrade timm # Only for Gemma 3N\n",
    "# from huggingface_hub import login\n",
    "# login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "88815e51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      "ðŸ¦¥ Unsloth Zoo will now patch everything to make training faster!\n",
      "==((====))==  Unsloth 2025.8.1: Fast Gemma3N patching. Transformers: 4.55.0.\n",
      "   \\\\   /|    NVIDIA L4. Num GPUs = 1. Max memory: 22.034 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.7.1+cu126. CUDA: 8.9. CUDA Toolkit: 12.6. Triton: 3.3.1\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.31.post1. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n",
      "Unsloth: Gemma3N does not support SDPA - switching to eager!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a81e6e35e20471b8ccf05faa310f1aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Get the model from Unsloth\n",
    "from unsloth import FastLanguageModel, FastModel\n",
    "import torch\n",
    "\n",
    "torch._dynamo.config.cache_size_limit = 64  # or higher  \n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = \"unsloth/gemma-3n-E4B-it-unsloth-bnb-4bit\",\n",
    "    dtype = None, # None for auto detection\n",
    "    max_seq_length = 2048, # Used for training for function call responses\n",
    "    load_in_4bit = True,  # 4 bit quantization to reduce memory\n",
    "    full_finetuning = False, \n",
    "    attn_implementation=\"flash_attention\", \n",
    "    # token = \"hf_...\", # use one if using gated models\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "059fe73a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use this chat template for training tool calls\n",
    "tokenizer.chat_template = (\n",
    "    \"{{ bos_token }}{% for message in messages %}{% if message['role'] != 'system' %}{{ '<start_of_turn>' + message['role'] + '\\n' + message['content'] | trim + '<end_of_turn><eos>\\n' }}{% endif %}{% endfor %}{% if add_generation_prompt %}{{'<start_of_turn>model\\n'}}{% endif %}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d7380e05",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth: Dropout = 0 is supported for fast patching. You are using dropout = 0.05.\n",
      "Unsloth will patch all other layers, except LoRA matrices, causing a performance hit.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Making `model.base_model.model.model.language_model` require gradients\n"
     ]
    }
   ],
   "source": [
    "# Get the finetuning model\n",
    "from peft import TaskType\n",
    "\n",
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    finetune_vision_layers     = False, # Turn off for just text!\n",
    "    finetune_language_layers   = True,  # Should leave on!\n",
    "    finetune_attention_modules = True,  # Attention good for GRPO\n",
    "    finetune_mlp_modules       = True,  # Should leave on always!\n",
    "    target_modules=[\"gate_proj\",\"q_proj\",\"o_proj\",\"k_proj\",\"down_proj\",\"up_proj\",\"v_proj\"],\n",
    "    use_gradient_checkpointing=\"unsloth\",\n",
    "    task_type = TaskType.CAUSAL_LM, \n",
    "    bias= \"none\",\n",
    "    use_rslora=False,\n",
    "    loftq_config=None,\n",
    "    r = 16,\n",
    "    lora_alpha = 64,\n",
    "    lora_dropout = 0.05,    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "33d5b0c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'conversations': [{'content': \"You are a function calling AI model. You are provided with function signatures within <tools></tools> XML tags.You may call one or more functions to assist with the user query. Don't make assumptions about what values to plug into functions.Here are the available tools:<tools> [{'type': 'function', 'function': {'name': 'search_book', 'description': 'Search for a book based on title and/or author', 'parameters': {'type': 'object', 'properties': {'title': {'type': 'string', 'description': 'The title of the book'}, 'author': {'type': 'string', 'description': 'The author of the book'}}, 'required': []}}}, {'type': 'function', 'function': {'name': 'get_definition', 'description': 'Get the definition of a word', 'parameters': {'type': 'object', 'properties': {'word': {'type': 'string', 'description': 'The word to get the definition for'}}, 'required': ['word']}}}] </tools>Use the following pydantic model json schema for each tool call you will make: {'title': 'FunctionCall', 'type': 'object', 'properties': {'arguments': {'title': 'Arguments', 'type': 'object'}, 'name': {'title': 'Name', 'type': 'string'}}, 'required': ['arguments', 'name']}For each function call return a json object with function name and arguments within <tool_call></tool_call> XML tags as follows:\\n<tool_call>\\n{tool_call}\\n</tool_call>Also, before making a call to a function take the time to plan the function to take. Make that thinking process between <think>{your thoughts}</think>\\n\\nCan you help me find a book?\",\n",
       "   'role': 'human'},\n",
       "  {'content': 'Of course! Could you please provide me with the title or the author of the book?',\n",
       "   'role': 'model'},\n",
       "  {'content': 'The book is called \"1984\" and it\\'s written by George Orwell.',\n",
       "   'role': 'human'},\n",
       "  {'content': \"<tool_call>\\n{'name': 'search_book', 'arguments': {'title': '1984', 'author': 'George Orwell'}}\\n</tool_call>\",\n",
       "   'role': 'model'},\n",
       "  {'content': \"<tool_response>\\n{'status': 'success', 'data': {'title': '1984', 'author': 'George Orwell', 'publisher': 'Secker & Warburg', 'publication_year': '1949', 'genre': 'Dystopian, political fiction, social science fiction'}}\\n</tool_response>\",\n",
       "   'role': 'tool'},\n",
       "  {'content': 'I found the book you were looking for. \"1984\" by George Orwell was published by Secker & Warburg in 1949. It falls under the genres of Dystopian, political fiction, and social science fiction.',\n",
       "   'role': 'model'}]}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the datasets\n",
    "from datasets import load_dataset\n",
    "dataset = load_dataset(\"lmassaron/hermes-function-calling-v1\", split=\"train\")\n",
    "eval_dataset = load_dataset(\"lmassaron/hermes-function-calling-v1\", split=\"test\")\n",
    "dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d6251aea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'conversations': [{'content': \"You are a function calling AI model. You are provided with function signatures within <tools></tools> XML tags.You may call one or more functions to assist with the user query. Don't make assumptions about what values to plug into functions.Here are the available tools:<tools> [{'type': 'function', 'function': {'name': 'convert_currency', 'description': 'Convert currency from one type to another', 'parameters': {'type': 'object', 'properties': {'amount': {'type': 'number', 'description': 'The amount to be converted'}, 'from_currency': {'type': 'string', 'description': 'The currency to convert from'}, 'to_currency': {'type': 'string', 'description': 'The currency to convert to'}}, 'required': ['amount', 'from_currency', 'to_currency']}}}, {'type': 'function', 'function': {'name': 'create_note', 'description': 'Create a new note', 'parameters': {'type': 'object', 'properties': {'title': {'type': 'string', 'description': 'The title of the note'}, 'content': {'type': 'string', 'description': 'The content of the note'}}, 'required': ['title', 'content']}}}] </tools>Use the following pydantic model json schema for each tool call you will make: {'title': 'FunctionCall', 'type': 'object', 'properties': {'arguments': {'title': 'Arguments', 'type': 'object'}, 'name': {'title': 'Name', 'type': 'string'}}, 'required': ['arguments', 'name']}For each function call return a json object with function name and arguments within <tool_call></tool_call> XML tags as follows:\\n<tool_call>\\n{tool_call}\\n</tool_call>Also, before making a call to a function take the time to plan the function to take. Make that thinking process between <think>{your thoughts}</think>\\n\\nHi, I need to convert 500 US dollars to Euros. Can you help me with that?\",\n",
       "   'role': 'human'},\n",
       "  {'content': \"<tool_call>\\n{'name': 'convert_currency', 'arguments': {'amount': 500, 'from_currency': 'USD', 'to_currency': 'EUR'}}\\n</tool_call>\",\n",
       "   'role': 'model'},\n",
       "  {'content': \"<tool_response>\\n{'converted_amount': 425.5, 'from_currency': 'USD', 'to_currency': 'EUR'}\\n</tool_response>\",\n",
       "   'role': 'tool'},\n",
       "  {'content': 'Sure, 500 US dollars is approximately 425.50 Euros.',\n",
       "   'role': 'model'}],\n",
       " 'text': \"<start_of_turn>human\\nYou are a function calling AI model. You are provided with function signatures within <tools></tools> XML tags.You may call one or more functions to assist with the user query. Don't make assumptions about what values to plug into functions.Here are the available tools:<tools> [{'type': 'function', 'function': {'name': 'convert_currency', 'description': 'Convert currency from one type to another', 'parameters': {'type': 'object', 'properties': {'amount': {'type': 'number', 'description': 'The amount to be converted'}, 'from_currency': {'type': 'string', 'description': 'The currency to convert from'}, 'to_currency': {'type': 'string', 'description': 'The currency to convert to'}}, 'required': ['amount', 'from_currency', 'to_currency']}}}, {'type': 'function', 'function': {'name': 'create_note', 'description': 'Create a new note', 'parameters': {'type': 'object', 'properties': {'title': {'type': 'string', 'description': 'The title of the note'}, 'content': {'type': 'string', 'description': 'The content of the note'}}, 'required': ['title', 'content']}}}] </tools>Use the following pydantic model json schema for each tool call you will make: {'title': 'FunctionCall', 'type': 'object', 'properties': {'arguments': {'title': 'Arguments', 'type': 'object'}, 'name': {'title': 'Name', 'type': 'string'}}, 'required': ['arguments', 'name']}For each function call return a json object with function name and arguments within <tool_call></tool_call> XML tags as follows:\\n<tool_call>\\n{tool_call}\\n</tool_call>Also, before making a call to a function take the time to plan the function to take. Make that thinking process between <think>{your thoughts}</think>\\n\\nHi, I need to convert 500 US dollars to Euros. Can you help me with that?<end_of_turn><eos>\\n<start_of_turn>model\\n<tool_call>\\n{'name': 'convert_currency', 'arguments': {'amount': 500, 'from_currency': 'USD', 'to_currency': 'EUR'}}\\n</tool_call><end_of_turn><eos>\\n<start_of_turn>tool\\n<tool_response>\\n{'converted_amount': 425.5, 'from_currency': 'USD', 'to_currency': 'EUR'}\\n</tool_response><end_of_turn><eos>\\n<start_of_turn>model\\nSure, 500 US dollars is approximately 425.50 Euros.<end_of_turn><eos>\\n\"}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert the dataset to the correct format for finetuning\n",
    "def formatting_prompts_func(examples):\n",
    "   convos = examples[\"conversations\"]\n",
    "   texts = [tokenizer.apply_chat_template(convo, tokenize = False, add_generation_prompt = False).removeprefix('<bos>') for convo in convos]\n",
    "   return { \"text\" : texts, }\n",
    "\n",
    "dataset =  dataset.map(formatting_prompts_func, batched = True)\n",
    "eval_dataset =  eval_dataset.map(formatting_prompts_func, batched = True)\n",
    "eval_dataset[20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ee460c5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup the fine-tuning trainer\n",
    "from unsloth import is_bfloat16_supported\n",
    "training_arguments = {\n",
    "    # Basic training configuration\n",
    "    \"num_train_epochs\": 1,\n",
    "    #\"bf16\": is_bfloat16_supported(),  # Use bfloat16 if supported\n",
    "    #\"fp16\": not is_bfloat16_supported(),  # Use if bfloat16 not supported\n",
    "    # \"max_steps\": 240,\n",
    "    # Evaluation and saving\n",
    "    \"eval_strategy\": \"epoch\",\n",
    "    \"save_strategy\": \"epoch\",\n",
    "    \"per_device_train_batch_size\": 1,\n",
    "    \"per_device_eval_batch_size\": 1,\n",
    "    \"gradient_accumulation_steps\": 4,\n",
    "    # Optimization settings\n",
    "    \"optim\": \"adamw_torch_fused\",\n",
    "    \"learning_rate\": 1e-4,\n",
    "    \"weight_decay\": 0.1,\n",
    "    \"max_grad_norm\": 1.0,\n",
    "    \"lr_scheduler_type\": \"cosine\",\n",
    "    \"warmup_ratio\": 0.1,\n",
    "    # Memory optimization\n",
    "    #\"save_total_limit\": 2,\n",
    "    #\"greater_is_better\": False,\n",
    "    # Logging and output\n",
    "    \"logging_steps\": 1,\n",
    "    #\"report_to\": None,\n",
    "    \"logging_dir\": \"logs/runs\",\n",
    "    \"overwrite_output_dir\": True,\n",
    "    # Model sharing\n",
    "    \"push_to_hub\": False,\n",
    "    \"hub_private_repo\": False,\n",
    "}\n",
    "\n",
    "from trl import SFTTrainer, SFTConfig\n",
    "trainer = SFTTrainer(\n",
    "    model = model,\n",
    "    tokenizer = tokenizer,\n",
    "    train_dataset = dataset,\n",
    "    eval_dataset= eval_dataset,\n",
    "    dataset_text_field = \"text\",\n",
    "    max_seq_length = 2048,\n",
    "    dataset_num_proc = 2,\n",
    "    packing = False,\n",
    "    bias = \"none\",\n",
    "    use_gradient_checkpointing = \"unsloth\",\n",
    "    args = SFTConfig(**training_arguments),  \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "75937357",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model on the responses only, ignore user instructions\n",
    "from unsloth.chat_templates import train_on_responses_only\n",
    "trainer = train_on_responses_only(\n",
    "    trainer,\n",
    "    instruction_part = \"<start_of_turn>user\\n\",\n",
    "    response_part = \"<start_of_turn>model\\n\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d9c2737e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"<bos><start_of_turn>human\\nYou are a function calling AI model. You are provided with function signatures within <tools></tools> XML tags.You may call one or more functions to assist with the user query. Don't make assumptions about what values to plug into functions.Here are the available tools:<tools> [{'type': 'function', 'function': {'name': 'analyze_website', 'description': 'Analyze the content and structure of a website', 'parameters': {'type': 'object', 'properties': {'url': {'type': 'string', 'description': 'The URL of the website to analyze'}}, 'required': ['url']}}}, {'type': 'function', 'function': {'name': 'calculate_bmi', 'description': 'Calculate the Body Mass Index (BMI)', 'parameters': {'type': 'object', 'properties': {'weight': {'type': 'number', 'description': 'The weight in kilograms'}, 'height': {'type': 'number', 'description': 'The height in meters'}}, 'required': ['weight', 'height']}}}] </tools>Use the following pydantic model json schema for each tool call you will make: {'title': 'FunctionCall', 'type': 'object', 'properties': {'arguments': {'title': 'Arguments', 'type': 'object'}, 'name': {'title': 'Name', 'type': 'string'}}, 'required': ['arguments', 'name']}For each function call return a json object with function name and arguments within <tool_call></tool_call> XML tags as follows:\\n<tool_call>\\n{tool_call}\\n</tool_call>Also, before making a call to a function take the time to plan the function to take. Make that thinking process between <think>{your thoughts}</think>\\n\\nHi, I need to analyze the structure and content of a website. Can you help me with that?<end_of_turn><eos>\\n<start_of_turn>model\\nOf course, I can help you with that. Could you please provide me with the URL of the website you want to analyze?<end_of_turn><eos>\\n<start_of_turn>human\\nSure, the website URL is www.example.com.<end_of_turn><eos>\\n<start_of_turn>model\\n<tool_call>\\n{'name': 'analyze_website', 'arguments': {'url': 'www.example.com'}}\\n</tool_call><end_of_turn><eos>\\n<start_of_turn>tool\\n<tool_response>\\n{'status': 'success', 'message': 'Website analysis completed', 'data': {'structure': 'The website has a clear and intuitive structure with a navigation menu at the top. The homepage, about us, services, and contact us pages are easily accessible.', 'content': 'The content is well-written and informative. It is updated regularly and is relevant to the target audience.'}}\\n</tool_response><end_of_turn><eos>\\n<start_of_turn>model\\nThe analysis of the website www.example.com is completed. The website has a clear and intuitive structure with a navigation menu at the top. The homepage, about us, services, and contact us pages are easily accessible. The content is well-written and informative. It is updated regularly and is relevant to the target audience.<end_of_turn><eos>\\n\""
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Verify the chat template was applied correctly. Only 1 <bos> token should be present.\n",
    "tokenizer.decode(trainer.train_dataset[100][\"input_ids\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d0a4bf65",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"                                                                                                                                                                                                                                                                                                                                                                                                                    Of course, I can help you with that. Could you please provide me with the URL of the website you want to analyze?<end_of_turn><eos>\\n<start_of_turn>human\\nSure, the website URL is www.example.com.<end_of_turn><eos>\\n<start_of_turn>model\\n<tool_call>\\n{'name': 'analyze_website', 'arguments': {'url': 'www.example.com'}}\\n</tool_call><end_of_turn><eos>\\n<start_of_turn>tool\\n<tool_response>\\n{'status': 'success', 'message': 'Website analysis completed', 'data': {'structure': 'The website has a clear and intuitive structure with a navigation menu at the top. The homepage, about us, services, and contact us pages are easily accessible.', 'content': 'The content is well-written and informative. It is updated regularly and is relevant to the target audience.'}}\\n</tool_response><end_of_turn><eos>\\n<start_of_turn>model\\nThe analysis of the website www.example.com is completed. The website has a clear and intuitive structure with a navigation menu at the top. The homepage, about us, services, and contact us pages are easily accessible. The content is well-written and informative. It is updated regularly and is relevant to the target audience.<end_of_turn><eos>\\n\""
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Verify user instruction masked\n",
    "tokenizer.decode([tokenizer.pad_token_id if x == -100 else x for x in trainer.train_dataset[100][\"labels\"]]).replace(tokenizer.pad_token, \" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "88247f1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n",
      "   \\\\   /|    Num examples = 4,167 | Num Epochs = 1 | Total steps = 1,042\n",
      "O^O/ \\_/ \\    Batch size per device = 1 | Gradient accumulation steps = 4\n",
      "\\        /    Data Parallel GPUs = 1 | Total batch size (1 x 4 x 1) = 4\n",
      " \"-____-\"     Trainable parameters = 40,189,952 of 7,890,168,144 (0.51% trained)\n",
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Will smartly offload gradients to save VRAM!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1042' max='1042' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1042/1042 1:50:33, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.111000</td>\n",
       "      <td>0.295569</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth: Not an error, but Gemma3nForConditionalGeneration does not accept `num_items_in_batch`.\n",
      "Using gradient accumulation will be very slightly less accurate.\n",
      "Read more on gradient accumulation issues here: https://unsloth.ai/blog/gradient\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1042, training_loss=0.5097231654246515, metrics={'train_runtime': 6663.7833, 'train_samples_per_second': 0.625, 'train_steps_per_second': 0.156, 'total_flos': 9.095523591867322e+16, 'train_loss': 0.5097231654246515})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the model\n",
    "# trainer.max_steps = 60\n",
    "trainer_stats = trainer.train()\n",
    "trainer_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6a6b7d3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sure, I can help with that. Could you please provide me with the title and description of the todo item?<end_of_turn>\n"
     ]
    }
   ],
   "source": [
    "from transformers import TextStreamer\n",
    "\n",
    "messages = [{\n",
    "    \"role\": \"user\",\n",
    "    \"content\": [{\n",
    "        \"type\" : \"text\",\n",
    "        \"text\" : eval_dataset[10][\"conversations\"][0][\"content\"],\n",
    "    }]\n",
    "}]\n",
    "# Convert the messages to the correct format for generation\n",
    "inputs = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    add_generation_prompt = True, # Must add for generation\n",
    "    return_tensors = \"pt\",\n",
    "    tokenize = True,\n",
    "    return_dict = True,\n",
    ").to(\"cuda\")\n",
    "\n",
    "_ = model.generate(\n",
    "    **inputs,\n",
    "    max_new_tokens = 64, # Increase for longer outputs!\n",
    "    # Recommended Gemma-3 settings!\n",
    "    temperature = 1.0, top_p = 0.95, top_k = 64,\n",
    "    streamer = TextStreamer(tokenizer, skip_prompt = True),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2e960965",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "model_name = \"gemma3n_e4b_tools_test\"\n",
    "model_name_gguf = model_name\n",
    "model_merged_hf_repo = \"allisterb/gemma3n_e4b_tools_test\"\n",
    "model_gguf_hf_repo = model_merged_hf_repo + \"-GGUF\"\n",
    "hf_token = os.environ[\"HF_TOKEN\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b44a5ba4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found HuggingFace hub cache directory: /home/eddsa-key-20250707/.cache/huggingface/hub\n",
      "Checking cache directory for required files...\n",
      "Cache check failed: model-00001-of-00004.safetensors not found in local cache.\n",
      "Not all required files found in cache. Will proceed with downloading.\n",
      "Downloading safetensors index for unsloth/gemma-3n-e4b-it...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth: Merging weights into 16bit:   0%|          | 0/4 [00:00<?, ?it/s]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b39250f9b30c42e1bd9e6ebcc931aa14",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00004.safetensors:   0%|          | 0.00/3.08G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth: Merging weights into 16bit:  25%|â–ˆâ–ˆâ–Œ       | 1/4 [00:42<02:06, 42.08s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "640381fbeaf94407a59727854c70e772",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00004.safetensors:   0%|          | 0.00/4.97G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth: Merging weights into 16bit:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 2/4 [01:57<02:03, 61.52s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "166cc8c46bfa4f28bdcbcec5583535ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00003-of-00004.safetensors:   0%|          | 0.00/4.99G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth: Merging weights into 16bit:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 3/4 [03:17<01:10, 70.03s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f7cc90685d04e0bb79b76f0d757720f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00004-of-00004.safetensors:   0%|          | 0.00/2.66G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth: Merging weights into 16bit: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [03:55<00:00, 58.87s/it]\n"
     ]
    }
   ],
   "source": [
    "# Save merged model\n",
    "model.save_pretrained_merged(model_name, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "26407b30",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No files have been modified since last commit. Skipping to prevent empty commit.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a1700457d568494880b8667d41d8aed0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing Files (0 / 0)                : |          |  0.00B /  0.00B            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e119f8a498754e5c9e4c63820051141a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "New Data Upload                         : |          |  0.00B /  0.00B            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "70a3950ea69a4c5997cec100111c5aa0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  /tmp/tmpoidft494/tokenizer.model      : 100%|##########| 4.70MB / 4.70MB            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a92259e89a2470da48f57dca1e5fdb1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  /tmp/tmpoidft494/tokenizer.json       : 100%|##########| 33.4MB / 33.4MB            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No files have been modified since last commit. Skipping to prevent empty commit.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found HuggingFace hub cache directory: /home/eddsa-key-20250707/.cache/huggingface/hub\n",
      "Checking cache directory for required files...\n",
      "Cache check failed: model-00001-of-00004.safetensors not found in local cache.\n",
      "Not all required files found in cache. Will proceed with downloading.\n",
      "Downloading safetensors index for unsloth/gemma-3n-e4b-it...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc454952fffe4164a5a9dc70ef2f0baa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No files have been modified since last commit. Skipping to prevent empty commit.\n",
      "Unsloth: Merging weights into 16bit:   0%|          | 0/4 [00:00<?, ?it/s]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5537df3c72cf4a0fb7b3f66a5022aa31",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00004.safetensors:   0%|          | 0.00/3.08G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "25ddc5bbf910491b9cd9f123d9f91a4e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing Files (0 / 0)                : |          |  0.00B /  0.00B            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2ffba9a3759f4d19b4dd7ba5a91d63e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "New Data Upload                         : |          |  0.00B /  0.00B            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb999ce50e024c93a371de871f97406a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  ...94/model-00001-of-00004.safetensors:   1%|1         | 41.8MB / 3.08GB            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No files have been modified since last commit. Skipping to prevent empty commit.\n",
      "Unsloth: Merging weights into 16bit:  25%|â–ˆâ–ˆâ–Œ       | 1/4 [01:07<03:22, 67.65s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc9c7046676c4d498478fa02ef7021ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00004.safetensors:   0%|          | 0.00/4.97G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c5c877aeba52461f9d0d313faaff3faf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing Files (0 / 0)                : |          |  0.00B /  0.00B            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "deb22151c84549c8a00319b2b8712e9c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "New Data Upload                         : |          |  0.00B /  0.00B            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d621aca00994c288d7a0d8d66c5a87a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  ...94/model-00002-of-00004.safetensors:   1%|          | 41.9MB / 4.97GB            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth: Merging weights into 16bit:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 2/4 [03:18<03:29, 104.54s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d83b5c8bb39146f795142a3fa104f766",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00003-of-00004.safetensors:   0%|          | 0.00/4.99G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f2990ad0a2684b6884387d4ced5f6d90",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing Files (0 / 0)                : |          |  0.00B /  0.00B            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cba3766fe0cb41769ab1b044e8d54e9c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "New Data Upload                         : |          |  0.00B /  0.00B            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "755f508d9702459b967a50806f6cbab5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  ...94/model-00003-of-00004.safetensors:   0%|          |  602kB / 4.99GB            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth: Merging weights into 16bit:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 3/4 [05:32<01:58, 118.21s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "07896d2af24d4968b04c50a54eea97a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00004-of-00004.safetensors:   0%|          | 0.00/2.66G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e8bced6fc27c46c588e20d14352e6c19",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing Files (0 / 0)                : |          |  0.00B /  0.00B            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2753db2e73de4c229e8cd5e2966e3a8e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "New Data Upload                         : |          |  0.00B /  0.00B            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c48a5079b05b421cb4f9efaee82fc517",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  ...94/model-00004-of-00004.safetensors:   0%|          | 36.1kB / 2.66GB            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth: Merging weights into 16bit: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [06:43<00:00, 100.78s/it]\n"
     ]
    }
   ],
   "source": [
    "# Push the merged model to Hugging Face\n",
    "model.push_to_hub_merged(model_merged_hf_repo, tokenizer, token = hf_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "08da9351",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth GGUF:hf-to-gguf:Loading model: gemma3n_e4b_tools_test\n",
      "Unsloth GGUF:hf-to-gguf:Model architecture: Gemma3nForConditionalGeneration\n",
      "Unsloth GGUF:gguf.gguf_writer:gguf: This GGUF file is for Little Endian only\n",
      "Unsloth GGUF:hf-to-gguf:Exporting model...\n",
      "Unsloth GGUF:hf-to-gguf:gguf: loading model weight map from 'model.safetensors.index.json'\n",
      "Unsloth GGUF:hf-to-gguf:gguf: loading model part 'model-00001-of-00004.safetensors'\n",
      "Unsloth GGUF:hf-to-gguf:altup_proj.weight,                 torch.bfloat16 --> Q8_0, shape = {2048, 2048, 3}\n",
      "Unsloth GGUF:hf-to-gguf:altup_unembd_proj.weight,          torch.bfloat16 --> Q8_0, shape = {2048, 2048, 3}\n",
      "Unsloth GGUF:hf-to-gguf:token_embd.weight,                 torch.bfloat16 --> Q8_0, shape = {2048, 262144}\n",
      "Unsloth GGUF:hf-to-gguf:gguf: loading model part 'model-00002-of-00004.safetensors'\n",
      "Unsloth GGUF:hf-to-gguf:per_layer_token_embd.weight,       torch.bfloat16 --> Q8_0, shape = {8960, 262144}\n",
      "Unsloth GGUF:hf-to-gguf:output_norm.weight,                torch.bfloat16 --> F32, shape = {2048}\n",
      "Unsloth GGUF:hf-to-gguf:per_layer_model_proj.weight,       torch.bfloat16 --> Q8_0, shape = {2048, 8960}\n",
      "Unsloth GGUF:hf-to-gguf:per_layer_proj_norm.weight,        torch.bfloat16 --> F32, shape = {256}\n",
      "Unsloth GGUF:hf-to-gguf:gguf: loading model part 'model-00003-of-00004.safetensors'\n",
      "Unsloth GGUF:hf-to-gguf:gguf: loading model part 'model-00004-of-00004.safetensors'\n",
      "Unsloth GGUF:hf-to-gguf:Set meta model\n",
      "Unsloth GGUF:hf-to-gguf:Set model parameters\n",
      "Unsloth GGUF:hf-to-gguf:Set model quantization version\n",
      "Unsloth GGUF:hf-to-gguf:Set model tokenizer\n",
      "Unsloth GGUF:gguf.vocab:Setting special token type bos to 2\n",
      "Unsloth GGUF:gguf.vocab:Setting special token type eos to 106\n",
      "Unsloth GGUF:gguf.vocab:Setting special token type unk to 3\n",
      "Unsloth GGUF:gguf.vocab:Setting special token type pad to 0\n",
      "Unsloth GGUF:gguf.vocab:Setting add_bos_token to True\n",
      "Unsloth GGUF:gguf.vocab:Setting add_sep_token to False\n",
      "Unsloth GGUF:gguf.vocab:Setting add_eos_token to False\n",
      "Unsloth GGUF:gguf.vocab:Setting chat_template to {{ bos_token }}{% for message in messages %}{% if message['role'] != 'system' %}{{ '<start_of_turn>' + message['role'] + '\n",
      "' + message['content'] | trim + '<end_of_turn><eos>\n",
      "' }}{% endif %}{% endfor %}{% if add_generation_prompt %}{{'<start_of_turn>model\n",
      "'}}{% endif %}\n",
      "Unsloth GGUF:gguf.gguf_writer:Writing the following files:\n",
      "Unsloth GGUF:gguf.gguf_writer:gemma3n_e4b_tools_test.Q8_0.gguf: n_tensors = 847, total_size = 7.3G\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c2b7af7a8b57407da20336f1803cd08f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Unsloth: GGUF conversion:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth GGUF:hf-to-gguf:Model successfully exported to ./\n",
      "Unsloth: Converted to gemma3n_e4b_tools_test.Q8_0.gguf with size = 7.3G\n",
      "Unsloth: Successfully saved GGUF to:\n",
      "gemma3n_e4b_tools_test.Q8_0.gguf\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['gemma3n_e4b_tools_test.Q8_0.gguf']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save to GGUF format\n",
    "model.save_pretrained_gguf(\n",
    "    model_name,\n",
    "    \"Q8_0\", # For now only Q8_0, BF16, F16 supported\n",
    "    tokenizer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "edc97a7d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d87131d636144566a8dd0700d6e1ff01",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing Files (0 / 0)                : |          |  0.00B /  0.00B            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d6373990eeae4f8084ab3b55a25f84bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "New Data Upload                         : |          |  0.00B /  0.00B            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d1c0f710dbb64e14b66c1503cc133976",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  gemma3n_e4b_tools_test.Q8_0.gguf      :   1%|          | 41.8MB / 7.30GB            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/allisterb/gemma3n_e4b_tools_test-GGUF/commit/5e9d90bb56e7092af7e9285e3753ed654808ce95', commit_message='(Trained with Unsloth)', commit_description='', oid='5e9d90bb56e7092af7e9285e3753ed654808ce95', pr_url=None, repo_url=RepoUrl('https://huggingface.co/allisterb/gemma3n_e4b_tools_test-GGUF', endpoint='https://huggingface.co', repo_type='model', repo_id='allisterb/gemma3n_e4b_tools_test-GGUF'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Upload GGUF model to Hugging Face\n",
    "from huggingface_hub import HfApi\n",
    "hf_api = HfApi()\n",
    "hf_api.upload_file(\n",
    "                path_or_fileobj = model_name +\".Q8_0.gguf\",\n",
    "                path_in_repo    = model_name +\".Q8_0.gguf\",\n",
    "                repo_id         = model_gguf_hf_repo,\n",
    "                repo_type       = \"model\",\n",
    "                commit_message  = \"(Trained with Unsloth)\",\n",
    "                token = hf_token,\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "15859de2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No files have been modified since last commit. Skipping to prevent empty commit.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/allisterb/gemma3n_e4b_tools_test-GGUF/commit/5e9d90bb56e7092af7e9285e3753ed654808ce95', commit_message='Upload template with huggingface_hub', commit_description='', oid='5e9d90bb56e7092af7e9285e3753ed654808ce95', pr_url=None, repo_url=RepoUrl('https://huggingface.co/allisterb/gemma3n_e4b_tools_test-GGUF', endpoint='https://huggingface.co', repo_type='model', repo_id='allisterb/gemma3n_e4b_tools_test-GGUF'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Upload Ollama template to Hugging Face Hub\n",
    "hf_api.upload_file(\n",
    "    path_or_fileobj=\"template\",\n",
    "    path_in_repo=\"template\",\n",
    "    repo_id=model_gguf_hf_repo,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dina",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
